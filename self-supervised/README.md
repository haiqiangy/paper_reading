# self-supervised learning
## Contrasive Learning
### 2020
- [**moco v1**] Momentum Contrast for Unsupervised Visual Representation Learning
[CVPR'2020] [[paper]](https://arxiv.org/pdf/1911.05722.pdf) [[code]](https://github.com/facebookresearch/moco)
- [**moco v2**] Improved Baselines with Momentum Contrastive Learning [Arxiv'2020] [[paper]](https://arxiv.org/pdf/2003.04297.pdf) [[code]](https://github.com/facebookresearch/moco)
- [**SimCLR**] A Simple Framework for Contrastive Learning of Visual Representations [ICML'2020] [[paper]](https://arxiv.org/pdf/2002.05709.pdf) [[code]](https://github.com/google-research/simclr)
- [**BYOL**] Bootstrap your own latent: A new approach to self-supervised Learning [NeurIPS'2020] [[paper]](https://arxiv.org/pdf/2006.07733.pdf) [[code]](https://github.com/deepmind/deepmind-research/tree/master/byol)
- [**SwAV**] Unsupervised Learning of Visual Features
by Contrasting Cluster Assignments [NeurIPS'2020] [[paper]](https://arxiv.org/pdf/2006.09882.pdf) [[code]](https://github.com/facebookresearch/swav)
### 2021
- [**moco v3**] An Empirical Study of Training Self-Supervised Vision Transformers [ICCV'2021] [[paper]](https://arxiv.org/pdf/2104.02057.pdf) [[code]](https://github.com/facebookresearch/moco-v3)
### 2022
- [**ContrastiveCrop**] Crafting Better Contrastive Views for Siamese Representation Learning [CVPR'2022] [[paper]](https://arxiv.org/abs/2202.03278) [[code]](https://github.com/xyupeng/ContrastiveCrop)
## Auto encoder
### 2021
- [**MAE**] Masked Autoencoders Are Scalable Vision Learners [[paper]](https://arxiv.org/pdf/2111.06377.pdf) [[code]](https://github.com/facebookresearch/mae)
- [**SimMiM**] SimMIM: a Simple Framework for Masked Image Modeling [[paper]](https://arxiv.org/pdf/2111.09886.pdf) [[code]](https://github.com/microsoft/SimMIM)
- [**CAE**] Context Autoencoder for Self-Supervised Representation Learning [[paper]](https://arxiv.org/pdf/2202.03026.pdf) [[code]](https://github.com/open-mmlab/mmselfsup/tree/master/configs/selfsup/cae)
### 2022
- [**ConvMAE**] ConvMAE: Masked Convolution Meets Masked Autoencoders [[paper]](https://arxiv.org/abs/2205.03892) [[code]](https://github.com/Alpha-VL/ConvMAE)
- [**FastConvMAE**] Fast ConvMAE: Fast Pretraining of ConvMAE [[code]](https://github.com/Alpha-VL/FastConvMAE)
## multimodal
- [**M3AE**] Multimodal Masked Autoencoders
Learn Transferable Representations [[paper]](https://arxiv.org/abs/2205.14204)
